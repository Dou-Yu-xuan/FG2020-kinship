{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./track3-data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract,Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.backend import sqrt\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from tqdm import tqdm_notebook, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.vggface import VGGFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val(familly_name):\n",
    "    train_file_path = \"./track3-data/train-val-data.xlsx\"\n",
    "    train_folders_path = \"./track3-data/train-val-faces/\"\n",
    "    val_famillies = familly_name\n",
    "\n",
    "    all_images = glob(train_folders_path + \"*/*/*.jpg\")\n",
    "    all_images=[x.replace('\\\\','/') for x in all_images]\n",
    "    train_images = [x for x in all_images if val_famillies not in x]\n",
    "    val_images = [x for x in all_images if val_famillies in x]\n",
    "\n",
    "    train_person_to_images_map = defaultdict(list)\n",
    "    # \"./track2-data/train-faces/F0001/MID1/P00001_face0.jpg\"\n",
    "    ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
    "\n",
    "    for x in train_images:\n",
    "        train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "    val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "    for x in val_images:\n",
    "        val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "    relationships = pd.read_excel(train_file_path)\n",
    "    relationships = list(zip(relationships.p1.values, relationships.p2.values))\n",
    "    relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n",
    "\n",
    "    train = [x for x in relationships if val_famillies not in x[0]]\n",
    "    val = [x for x in relationships if val_famillies in x[0]]\n",
    "    return train, val, train_person_to_images_map, val_person_to_images_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signed_sqrt(x):\n",
    "    return keras.backend.sign(x) * keras.backend.sqrt(keras.backend.abs(x) + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return preprocess_input(img, version=2)\n",
    "\n",
    "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size // 2)\n",
    "        labels = [1] * len(batch_tuples)\n",
    "        while len(batch_tuples) < batch_size:\n",
    "            p1 = choice(ppl)\n",
    "            p2 = choice(ppl)\n",
    "\n",
    "            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
    "                batch_tuples.append((p1, p2))\n",
    "                labels.append(0)\n",
    "\n",
    "        for x in batch_tuples:\n",
    "            if not len(person_to_images_map[x[0]]):\n",
    "                print(x[0])\n",
    "\n",
    "        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
    "        X1 = np.array([read_img(x) for x in X1])\n",
    "\n",
    "        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
    "        X2 = np.array([read_img(x) for x in X2])\n",
    "\n",
    "        yield [X1, X2], labels\n",
    "\n",
    "\n",
    "def baseline_model():\n",
    "    input_1 = Input(shape=(224, 224, 3))\n",
    "    input_2 = Input(shape=(224, 224, 3))\n",
    "\n",
    "    base_model = VGGFace(model='resnet50', include_top=False)\n",
    "\n",
    "    for x in base_model.layers[:-3]:\n",
    "        x.trainable = True\n",
    "\n",
    "    x1 = base_model(input_1)\n",
    "    x2 = base_model(input_2)\n",
    "\n",
    "    x1=GlobalMaxPool2D()(x1)\n",
    "    x2=GlobalAvgPool2D()(x2)\n",
    "    \n",
    "    # (x-y)^2 + (x^2-y^2) + xy\n",
    "    x3 = Subtract()([x1, x2])\n",
    "    x3 = Multiply()([x3, x3])\n",
    "    x1_ = Multiply()([x1, x1])\n",
    "    x2_ = Multiply()([x2, x2])\n",
    "    x4 = Subtract()([x1_, x2_])\n",
    "    x5 = Multiply()([x1, x2])\n",
    "    \n",
    "    # (x-y) + (x+y) + xy\n",
    "#     x3 = Subtract()([x1, x2])\n",
    "#     x4 = Add()([x1,x2])\n",
    "#     x5 = Multiply()([x1, x2])\n",
    "    \n",
    "    x = Concatenate(axis=-1)([x3, x4, x5])\n",
    "\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.02)(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1, input_2], out)\n",
    "\n",
    "#     model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
    "    model.compile(loss=[focal_loss(alpha=.25, gamma=2)], metrics=['acc'], optimizer=Adam(0.00003))\n",
    "#     model.compile(loss=[focal_loss(alpha=.25, gamma=2)], metrics=['acc'], optimizer=Adam(0.00001))\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_famillies_list = [\"F01\",\"F03\",\"F05\",\"F07\", \"F09\"]\n",
    "n_val_famillies_list = len(val_famillies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm_notebook(range(n_val_famillies_list)):\n",
    "    train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_famillies_list[i])\n",
    "    file_path = f\"vgg_face_{i}.h5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.3, patience=30, verbose=1)\n",
    "#     reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.2, patience=20, verbose=1)   \n",
    "    callbacks_list = [checkpoint, reduce_on_plateau]\n",
    "\n",
    "    history = model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), \n",
    "                                  use_multiprocessing=False,\n",
    "                                  validation_data=gen(val, val_person_to_images_map, batch_size=16), \n",
    "                                  epochs=66, verbose=2,\n",
    "                                  workers=1, callbacks=callbacks_list, \n",
    "                                  steps_per_epoch=300, validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sort_key(s):\n",
    "    if s:\n",
    "        try:\n",
    "            c = re.findall('\\d+', s)[0]\n",
    "        except:\n",
    "            c = -1\n",
    "        return int(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# test multi models\n",
    "test_gallery_path = \"./track3-data/test-data/gallery/gallery_faces/\"\n",
    "test_probes_path = \"./track3-data/test-data/probes/probe_subjects/\"\n",
    "\n",
    "probes_id = os.listdir(test_probes_path)\n",
    "probes_id.sort(key=sort_key)\n",
    "\n",
    "for k in tqdm(range(len(probes_id))):\n",
    "    print(\"test---\"+probes_id[k])\n",
    "\n",
    "    submission = pd.read_csv(\"./track3-data/test-data/track3-test/track3-test-{}.csv\".format(probes_id[k]))\n",
    "\n",
    "    # face0.jpg-P07920_face1.jpg, s0\n",
    "\n",
    "    def chunker(seq, size=64):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "    preds_for_sub = np.zeros(submission.shape[0])\n",
    "\n",
    "    for i in tqdm_notebook(range(n_val_famillies_list)):\n",
    "        file_path = f\"./track3-models/01357-resnet50-bce-1/vgg_face_{i}.h5\"\n",
    "        model.load_weights(file_path)\n",
    "        # Get the predictions\n",
    "        predictions = []\n",
    "\n",
    "        for batch in tqdm_notebook(chunker(submission.img_pair.values)):\n",
    "            X1 = [x.split(\"-\")[0] for x in batch]\n",
    "            X1 = np.array([read_img(test_gallery_path + x) for x in X1])\n",
    "\n",
    "            X2 = [x.split(\"-\")[1] for x in batch]\n",
    "            X2 = np.array([read_img(test_probes_path + probes_id[k] + '/' + x) for x in X2])\n",
    "\n",
    "            pred = model.predict([X1, X2]).ravel().tolist()\n",
    "            predictions += pred\n",
    "        preds_for_sub += np.array(predictions) / n_val_famillies_list\n",
    "\n",
    "    submission['score'] = preds_for_sub\n",
    "    submission.to_csv(\"./track3-res/sim/track3-sim-{}.csv\".format(probes_id[k]), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# test single model\n",
    "test_gallery_path = \"./track3-data/test-data/gallery/gallery_faces/\"\n",
    "test_probes_path = \"./track3-data/test-data/probes/probe_subjects/\"\n",
    "\n",
    "probes_id = os.listdir(test_probes_path)\n",
    "probes_id.sort(key=sort_key)\n",
    "\n",
    "file_path = \"./track3-models/13579-resnet50-bce-1/vgg_face_0.h5\"\n",
    "model.load_weights(file_path)\n",
    "\n",
    "for k in tqdm(range(len(probes_id))):\n",
    "    print(\"test---\"+probes_id[k])\n",
    "    \n",
    "    submission = pd.read_csv(\"./track3-data/test-data/track3-test/track3-test-{}.csv\".format(probes_id[k]))\n",
    "\n",
    "    # face0.jpg-P07920_face1.jpg, s0\n",
    "\n",
    "    def chunker(seq, size=256):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "    preds_for_sub = np.zeros(submission.shape[0])\n",
    "\n",
    "#     file_path = \"./track3-models/13579-resnet50-bce-1/vgg_face_0.h5\"\n",
    "#     model.load_weights(file_path)\n",
    "    # Get the predictions\n",
    "    predictions = []\n",
    "\n",
    "    for batch in tqdm_notebook(chunker(submission.img_pair.values)):\n",
    "        X1 = [x.split(\"-\")[0] for x in batch]\n",
    "        X1 = np.array([read_img(test_gallery_path + x) for x in X1])\n",
    "\n",
    "        X2 = [x.split(\"-\")[1] for x in batch]\n",
    "        X2 = np.array([read_img(test_probes_path + probes_id[k] + '/' + x) for x in X2])\n",
    "\n",
    "        pred = model.predict([X1, X2]).ravel().tolist()\n",
    "        predictions += pred\n",
    "    preds_for_sub += np.array(predictions)\n",
    "\n",
    "    submission['score'] = preds_for_sub\n",
    "    submission.to_csv(\"./track3-preds/13579/sim/track3-sim-{}.csv\".format(probes_id[k]), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "test_gallery_path = \"./track3-data/test-data/gallery/gallery_faces/\"\n",
    "test_probes_path = \"./track3-data/test-data/probes/probe_subjects/\"\n",
    "\n",
    "probes_id = os.listdir(test_probes_path)\n",
    "# probes_id.sort(key=sort_key)\n",
    "\n",
    "nums_gallery = 3897\n",
    "nums_probes_id = 190\n",
    "\n",
    "for k in range(nums_probes_id):\n",
    "    sim_lines = pd.read_csv(\"./track3-preds/24689/sim/track3-sim-{}.csv\".format(probes_id[k]))['score']\n",
    "    nums_probe_pre_id = len(sim_lines)//nums_gallery\n",
    "    sim2_dict = {}\n",
    "    for i in range(nums_gallery):\n",
    "        gallery_img = 'face{}.jpg'.format(str(i))\n",
    "        sim_score_list = [float(sim_lines[i*nums_probe_pre_id+j]) for j in range(nums_probe_pre_id)]\n",
    "        #================\n",
    "        sim2_score = np.mean(sim_score_list)\n",
    "        #================\n",
    "        sim2_dict[gallery_img] = sim2_score\n",
    "    sim2_sorted = sorted(sim2_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    rank_dict = {}\n",
    "    for item in zip(sim2_sorted, list(range(len(sim2_sorted)))):\n",
    "        name = item[0][0]\n",
    "        rank = item[1]\n",
    "        rank_dict[name] = rank\n",
    "        \n",
    "    sort_dict = {}\n",
    "    for item in zip(sim2_dict, sim2_sorted):\n",
    "        name = item[0]\n",
    "        sort = re.findall(r\"\\d+?\\d*\", item[1][0])[0]\n",
    "        sort_dict[name] = sort\n",
    "    \n",
    "    sim2 = defaultdict(list)\n",
    "    for item in sim2_dict:\n",
    "        sim2['gallery_img'].append(item)\n",
    "        sim2['score'].append(sim2_dict[item])\n",
    "        sim2['rank'].append(rank_dict[item])\n",
    "        sim2['sort'].append(sort_dict[item])\n",
    "    pd.DataFrame(sim2).to_csv(\"./track3-preds/24689/sim2/track3-sim2-{}.csv\".format(probes_id[k]))\n",
    "\n",
    "with open('./track3-preds/24689/sim2_predictions.csv','w',newline='') as f:\n",
    "    csv_write = csv.writer(f, dialect='excel')\n",
    "    for k in range(nums_probes_id):\n",
    "        sim2_lines = pd.read_csv(\"./track3-preds/24689/sim2/track3-sim2-{}.csv\".format(probes_id[k]))['sort']\n",
    "        csv_write.writerow(sim2_lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "import h5py\n",
    "\n",
    "test_gallery_path = \"./track3-data/test-data/gallery/gallery_faces/\"\n",
    "test_probes_path = \"./track3-data/test-data/probes/probe_subjects/\"\n",
    "\n",
    "gallery_img = os.listdir(test_gallery_path)\n",
    "gallery_img.sort(key=sort_key)\n",
    "probes_id = os.listdir(test_probes_path)\n",
    "probes_id.sort(key=sort_key)\n",
    "\n",
    "nums_gallery = 3897\n",
    "nums_probes_id = 190\n",
    "\n",
    "def chunker(seq, size=256):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def l2norm(x, eps=1e-6):\n",
    "    return x/(np.linalg.norm(x, ord=2, axis=1, keepdims=True)+eps)\n",
    "\n",
    "# get gallery features\n",
    "for i in range(5):\n",
    "    file_path = \"./track3-models/24689-resnet50-focalloss-1/vgg_face_{}.h5\".format(i)\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "    model_base_max = Model(inputs=model.get_layer('input_1').output, outputs=model.get_layer('global_max_pooling2d_1').output)\n",
    "    model_base_avg = Model(inputs=model.get_layer('input_2').output, outputs=model.get_layer('global_average_pooling2d_1').output)\n",
    "\n",
    "#     print(model_base_max.summary(), model_base_avg.summary()) \n",
    "\n",
    "    F_max_all = np.zeros((256, 2048))\n",
    "    F_avg_all = np.zeros((256, 2048))\n",
    "\n",
    "    for batch in tqdm_notebook(chunker(gallery_img)):\n",
    "        X = np.array([read_img(test_gallery_path + x) for x in batch])\n",
    "        F_max = model_base_max.predict(X)\n",
    "        F_avg = model_base_avg.predict(X)\n",
    "#         print(F_max.shape, F_avg.shape)\n",
    "        F_max_all = np.concatenate((F_max_all, F_max), axis=0)\n",
    "        F_avg_all = np.concatenate((F_avg_all, F_avg), axis=0)\n",
    "\n",
    "    F_max_avg_all = np.concatenate((F_max_all[256:], F_avg_all[256:]), axis=1)\n",
    "    F_max_all = l2norm(F_max_all[256:])\n",
    "    F_avg_all = l2norm(F_avg_all[256:])\n",
    "    F_max_avg_all = l2norm(F_max_avg_all)\n",
    "    print(F_max_all.shape, F_avg_all.shape, F_max_avg_all.shape)\n",
    "\n",
    "    with h5py.File(\"./track3-feats/24689/gallery/feat_gallery_{}.h5\".format(i), 'w') as f:\n",
    "        f.create_dataset('img_name', np.array(gallery_img).shape, dtype=h5py.special_dtype(vlen=str))[:] = np.array(gallery_img)\n",
    "        f['feat_max'] = F_max_all\n",
    "        f['feat_avg'] = F_avg_all\n",
    "        f['feat_max_avg'] = F_max_avg_all\n",
    "\n",
    "# get mean gallery features\n",
    "F_max_all_mean = np.zeros((nums_gallery, 2048))\n",
    "F_avg_all_mean = np.zeros((nums_gallery, 2048))\n",
    "F_max_avg_all_mean = np.zeros((nums_gallery, 4096))\n",
    "for i in range(5):\n",
    "    with h5py.File(\"./track3-feats/24689/gallery/feat_gallery_{}.h5\".format(i), 'r') as f:\n",
    "        F_max_all_mean += f['feat_max']\n",
    "        F_avg_all_mean += f['feat_avg']\n",
    "        F_max_avg_all_mean += f['feat_max_avg']\n",
    "with h5py.File(\"./track3-feats/24689/gallery/feat_gallery_mean.h5\", 'w') as f:\n",
    "    f.create_dataset('img_name', np.array(gallery_img).shape, dtype=h5py.special_dtype(vlen=str))[:] = np.array(gallery_img)\n",
    "    f['feat_max'] = F_max_all_mean/5.0\n",
    "    f['feat_avg'] = F_avg_all_mean/5.0\n",
    "    f['feat_max_avg'] = F_max_avg_all_mean/5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "import h5py\n",
    "\n",
    "test_gallery_path = \"./track3-data/test-data/gallery/gallery_faces/\"\n",
    "test_probes_path = \"./track3-data/test-data/probes/probe_subjects/\"\n",
    "\n",
    "gallery_img = os.listdir(test_gallery_path)\n",
    "gallery_img.sort(key=sort_key)\n",
    "probes_id = os.listdir(test_probes_path)\n",
    "probes_id.sort(key=sort_key)\n",
    "\n",
    "nums_gallery = 3897\n",
    "nums_probes_id = 190\n",
    "\n",
    "def chunker(seq, size=256):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def l2norm(x, eps=1e-6):\n",
    "    return x/(np.linalg.norm(x, ord=2, axis=1, keepdims=True)+eps)\n",
    "\n",
    "# get probes features\n",
    "for i in range(5):\n",
    "    file_path = \"./track3-models/24689-resnet50-focalloss-1/vgg_face_{}.h5\".format(i)\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "    model_base_max = Model(inputs=model.get_layer('input_1').output, outputs=model.get_layer('global_max_pooling2d_1').output)\n",
    "    model_base_avg = Model(inputs=model.get_layer('input_2').output, outputs=model.get_layer('global_average_pooling2d_1').output)\n",
    "\n",
    "    F_max_id = np.zeros((1, 2048))\n",
    "    F_avg_id = np.zeros((1, 2048))\n",
    "    F_max_avg_id = np.zeros((1, 4096))\n",
    "\n",
    "    for k in tqdm_notebook(range(nums_probes_id)):\n",
    "        probes_img = os.listdir(test_probes_path + probes_id[k])\n",
    "\n",
    "        F_max_all = np.zeros((256, 2048))\n",
    "        F_avg_all = np.zeros((256, 2048))\n",
    "\n",
    "        for batch in chunker(probes_img):\n",
    "            X = np.array([read_img(test_probes_path + probes_id[k] + '/' + x) for x in batch])\n",
    "            F_max = model_base_max.predict(X)\n",
    "            F_avg = model_base_avg.predict(X)\n",
    "#             print(F_max.shape, F_avg.shape)\n",
    "            F_max_all = np.concatenate((F_max_all, F_max), axis=0)\n",
    "            F_avg_all = np.concatenate((F_avg_all, F_avg), axis=0)\n",
    "\n",
    "        F_max_avg_all = np.concatenate((F_max_all[256:], F_avg_all[256:]), axis=1)\n",
    "        F_max_all = l2norm(F_max_all[256:])\n",
    "        F_avg_all = l2norm(F_avg_all[256:])\n",
    "        F_max_avg_all = l2norm(F_max_avg_all)\n",
    "#         print(F_max_all.shape, F_avg_all.shape, F_max_avg_all.shape)\n",
    "    \n",
    "        if not os.path.exists(\"./track3-feats/24689/probes/feat_probes_{}\".format(i)):\n",
    "            os.mkdir(\"./track3-feats/24689/probes/feat_probes_{}\".format(i))\n",
    "        with h5py.File(\"./track3-feats/24689/probes/feat_probes_{}/feat_probes_{}.h5\".format(i, probes_id[k]), 'w') as f:\n",
    "            f.create_dataset('img_name', np.array(probes_img).shape, dtype=h5py.special_dtype(vlen=str))[:] = np.array(probes_img)\n",
    "            f['feat_max'] = F_max_all\n",
    "            f['feat_avg'] = F_avg_all\n",
    "            f['feat_max_avg'] = F_max_avg_all\n",
    "\n",
    "        F_max_id = np.concatenate((F_max_id, np.mean(F_max_all, axis=0)[np.newaxis,:]), axis=0)\n",
    "        F_avg_id = np.concatenate((F_avg_id, np.mean(F_avg_all, axis=0)[np.newaxis,:]), axis=0)\n",
    "        F_max_avg_id = np.concatenate((F_max_avg_id, np.mean(F_max_avg_all, axis=0)[np.newaxis,:]), axis=0)\n",
    "#         print(F_max_id.shape, F_avg_id.shape, F_max_avg_id.shape)\n",
    "\n",
    "    with h5py.File(\"./track3-feats/24689/probes/feat_probes_{}.h5\".format(i), 'w') as f:\n",
    "        f.create_dataset('probes_id', np.array(probes_id).shape, dtype=h5py.special_dtype(vlen=str))[:] = np.array(probes_id)\n",
    "        f['feat_max'] = F_max_id[1:]\n",
    "        f['feat_avg'] = F_avg_id[1:]\n",
    "        f['feat_max_avg'] = F_max_avg_id[1:]\n",
    "    \n",
    "# get mean probes features\n",
    "F_max_id_mean = np.zeros((nums_probes_id, 2048))\n",
    "F_avg_id_mean = np.zeros((nums_probes_id, 2048))\n",
    "F_max_avg_id_mean = np.zeros((nums_probes_id, 4096))\n",
    "for i in range(5):\n",
    "    with h5py.File(\"./track3-feats/24689/probes/feat_probes_{}.h5\".format(i), 'r') as f:\n",
    "        F_max_id_mean += f['feat_max']\n",
    "        F_avg_id_mean += f['feat_avg']\n",
    "        F_max_avg_id_mean += f['feat_max_avg']\n",
    "with h5py.File(\"./track3-feats/24689/probes/feat_probes_mean.h5\", 'w') as f:\n",
    "    f.create_dataset('probes_id', np.array(probes_id).shape, dtype=h5py.special_dtype(vlen=str))[:] = np.array(probes_id)\n",
    "    f['feat_max'] = F_max_id_mean/5.0\n",
    "    f['feat_avg'] = F_avg_id_mean/5.0\n",
    "    f['feat_max_avg'] = F_max_avg_id_mean/5.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "test_gallery_path = \"./track3-data/test-data/gallery/gallery_faces/\"\n",
    "test_probes_path = \"./track3-data/test-data/probes/probe_subjects/\"\n",
    "\n",
    "def sort_key(s):\n",
    "    if s:\n",
    "        try:\n",
    "            c = re.findall('\\d+', s)[0]\n",
    "        except:\n",
    "            c = -1\n",
    "        return int(c)\n",
    "\n",
    "probes_id = os.listdir(test_probes_path)\n",
    "probes_id.sort(key=sort_key)\n",
    "    \n",
    "nums_gallery = 3897\n",
    "nums_probes_id = 190\n",
    "\n",
    "def read_feature(filepath):\n",
    "    h5f = h5py.File(filepath, 'r')\n",
    "    feat_max = h5f['feat_max'][:]\n",
    "    feat_avg = h5f['feat_avg'][:]\n",
    "    feat_max_avg = h5f['feat_max_avg'][:]\n",
    "    h5f.close()\n",
    "    return feat_max, feat_avg, feat_max_avg\n",
    "\n",
    "def calu_sim(feat_probes, feat_gallery):\n",
    "    return np.dot(feat_probes, feat_gallery.T)\n",
    "\n",
    "feat_gallery_max, feat_gallery_avg, feat_gallery_max_avg = read_feature(\"./track3-feats/24689/gallery/feat_gallery_mean.h5\")\n",
    "feat_probes_max, feat_probes_avg, feat_probes_max_avg = read_feature(\"./track3-feats/24689/probes/feat_probes_mean.h5\")\n",
    "\n",
    "feat_max_sim = calu_sim(feat_probes_max, feat_gallery_max)\n",
    "feat_avg_sim = calu_sim(feat_probes_avg, feat_gallery_avg)\n",
    "feat_max_avg_sim = calu_sim(feat_probes_max_avg, feat_gallery_max_avg)\n",
    "\n",
    "feat_max_sort = np.argsort(-feat_max_sim, axis=1)\n",
    "feat_avg_sort = np.argsort(-feat_avg_sim, axis=1)\n",
    "feat_max_avg_sort = np.argsort(-feat_max_avg_sim, axis=1)\n",
    "\n",
    "pd.DataFrame(feat_max_sort).to_csv(\"./track3-feats/24689/feat_max_pred_mean.csv\")\n",
    "pd.DataFrame(feat_avg_sort).to_csv(\"./track3-feats/24689/feat_avg_pred_mean.csv\")\n",
    "pd.DataFrame(feat_max_avg_sort).to_csv(\"./track3-feats/24689/feat_max_avg_pred_mean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "test_gallery_path = \"./track3-data/test-data/gallery/gallery_faces/\"\n",
    "test_probes_path = \"./track3-data/test-data/probes/probe_subjects/\"\n",
    "\n",
    "def sort_key(s):\n",
    "    if s:\n",
    "        try:\n",
    "            c = re.findall('\\d+', s)[0]\n",
    "        except:\n",
    "            c = -1\n",
    "        return int(c)\n",
    "\n",
    "probes_id = os.listdir(test_probes_path)\n",
    "probes_id.sort(key=sort_key)\n",
    "    \n",
    "nums_gallery = 3897\n",
    "nums_probes_id = 190\n",
    "\n",
    "def read_feature(filepath):\n",
    "    h5f = h5py.File(filepath, 'r')\n",
    "    feat_max = h5f['feat_max'][:]\n",
    "    feat_avg = h5f['feat_avg'][:]\n",
    "    feat_max_avg = h5f['feat_max_avg'][:]\n",
    "    h5f.close()\n",
    "    return feat_max, feat_avg, feat_max_avg\n",
    "\n",
    "def calu_sim(feat_probes, feat_gallery):\n",
    "    return np.dot(feat_probes, feat_gallery.T)\n",
    "\n",
    "feat_max_sim_mean = np.zeros((nums_probes_id, nums_gallery))\n",
    "feat_avg_sim_mean = np.zeros((nums_probes_id, nums_gallery))\n",
    "feat_max_avg_sim_mean = np.zeros((nums_probes_id, nums_gallery))\n",
    "\n",
    "for i in range(5):\n",
    "    feat_gallery_max, feat_gallery_avg, feat_gallery_max_avg = read_feature(\"./track3-feats/24689/gallery/feat_gallery_{}.h5\".format(i))\n",
    "    feat_probes_max, feat_probes_avg, feat_probes_max_avg = read_feature(\"./track3-feats/24689/probes/feat_probes_{}.h5\".format(i))\n",
    "\n",
    "    feat_max_sim = calu_sim(feat_probes_max, feat_gallery_max)\n",
    "    feat_avg_sim = calu_sim(feat_probes_avg, feat_gallery_avg)\n",
    "    feat_max_avg_sim = calu_sim(feat_probes_max_avg, feat_gallery_max_avg)\n",
    "\n",
    "    feat_max_sim_mean += feat_max_sim/5.0\n",
    "    feat_avg_sim_mean += feat_avg_sim/5.0\n",
    "    feat_max_avg_sim_mean += feat_max_avg_sim/5.0\n",
    "\n",
    "    feat_max_sort = np.argsort(-feat_max_sim, axis=1)\n",
    "    feat_avg_sort = np.argsort(-feat_avg_sim, axis=1)\n",
    "    feat_max_avg_sort = np.argsort(-feat_max_avg_sim, axis=1)\n",
    "\n",
    "    pd.DataFrame(feat_max_sort).to_csv(\"./track3-feats/24689/sub1/feat_max_pred_{}.csv\".format(i))\n",
    "    pd.DataFrame(feat_avg_sort).to_csv(\"./track3-feats/24689/sub1/feat_avg_pred_{}.csv\".format(i))\n",
    "    pd.DataFrame(feat_max_avg_sort).to_csv(\"./track3-feats/24689/sub1/feat_max_avg_pred_{}.csv\".format(i))\n",
    "\n",
    "feat_max_mean_sort = np.argsort(-feat_max_sim_mean, axis=1)\n",
    "feat_avg_mean_sort = np.argsort(-feat_avg_sim_mean, axis=1)\n",
    "feat_max_avg_mean_sort = np.argsort(-feat_max_avg_sim_mean, axis=1)\n",
    "\n",
    "pd.DataFrame(feat_max_mean_sort).to_csv(\"./track3-feats/24689/sub1/feat_max_pred.csv\")\n",
    "pd.DataFrame(feat_avg_mean_sort).to_csv(\"./track3-feats/24689/sub1/feat_avg_pred.csv\")\n",
    "pd.DataFrame(feat_max_avg_mean_sort).to_csv(\"./track3-feats/24689/sub1/feat_max_avg_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "test_gallery_path = \"./track3-data/test-data/gallery/gallery_faces/\"\n",
    "test_probes_path = \"./track3-data/test-data/probes/probe_subjects/\"\n",
    "\n",
    "def sort_key(s):\n",
    "    if s:\n",
    "        try:\n",
    "            c = re.findall('\\d+', s)[0]\n",
    "        except:\n",
    "            c = -1\n",
    "        return int(c)\n",
    "\n",
    "probes_id = os.listdir(test_probes_path)\n",
    "probes_id.sort(key=sort_key)\n",
    "    \n",
    "nums_gallery = 3897\n",
    "nums_probes_id = 190\n",
    "\n",
    "def read_feature(filepath):\n",
    "    h5f = h5py.File(filepath, 'r')\n",
    "    feat_max = h5f['feat_max'][:]\n",
    "    feat_avg = h5f['feat_avg'][:]\n",
    "    feat_max_avg = h5f['feat_max_avg'][:]\n",
    "    h5f.close()\n",
    "    return feat_max, feat_avg, feat_max_avg\n",
    "\n",
    "def calu_sim(feat_probes, feat_gallery):\n",
    "    return np.dot(feat_probes, feat_gallery.T)\n",
    "\n",
    "feat_max_sim_mean = np.zeros((nums_probes_id, nums_gallery))\n",
    "feat_avg_sim_mean = np.zeros((nums_probes_id, nums_gallery))\n",
    "feat_max_avg_sim_mean = np.zeros((nums_probes_id, nums_gallery))\n",
    "\n",
    "for idx in range(5):\n",
    "    feat_gallery_max, feat_gallery_avg, feat_gallery_max_avg = read_feature(\"./track3-feats/24689/gallery/feat_gallery_{}.h5\".format(idx))\n",
    "\n",
    "    feat_max_sim = np.zeros((1, nums_gallery))\n",
    "    feat_avg_sim = np.zeros((1, nums_gallery))\n",
    "    feat_max_avg_sim = np.zeros((1, nums_gallery))\n",
    "\n",
    "    for k in range(nums_probes_id):\n",
    "        feat_probes_max, feat_probes_avg, feat_probes_max_avg = read_feature(\"./track3-feats/24689/probes/feat_probes_{}/feat_probes_{}.h5\".format(idx, probes_id[k]))\n",
    "        feat_max_sim = np.concatenate((feat_max_sim, np.mean(calu_sim(feat_probes_max, feat_gallery_max), axis=0)[np.newaxis,:]), axis=0)\n",
    "        feat_avg_sim = np.concatenate((feat_avg_sim, np.mean(calu_sim(feat_probes_avg, feat_gallery_avg), axis=0)[np.newaxis,:]), axis=0)\n",
    "        feat_max_avg_sim = np.concatenate((feat_max_avg_sim, np.mean(calu_sim(feat_probes_max_avg, feat_gallery_max_avg), axis=0)[np.newaxis,:]), axis=0)\n",
    "#     print(feat_max_sim.shape)\n",
    "    feat_max_sim_mean += feat_max_sim[1:]/5.0\n",
    "    feat_avg_sim_mean += feat_avg_sim[1:]/5.0\n",
    "    feat_max_avg_sim_mean += feat_max_avg_sim[1:]/5.0\n",
    "\n",
    "    feat_max_sort = np.argsort(-feat_max_sim[1:], axis=1)\n",
    "    feat_avg_sort = np.argsort(-feat_avg_sim[1:], axis=1)\n",
    "    feat_max_avg_sort = np.argsort(-feat_max_avg_sim[1:], axis=1)\n",
    "\n",
    "    pd.DataFrame(feat_max_sort).to_csv(\"./track3-feats/24689/sub2/feat_max_pred_{}.csv\".format(idx))\n",
    "    pd.DataFrame(feat_avg_sort).to_csv(\"./track3-feats/24689/sub2/feat_avg_pred_{}.csv\".format(i))\n",
    "    pd.DataFrame(feat_max_avg_sort).to_csv(\"./track3-feats/24689/sub2/feat_max_avg_pred_{}.csv\".format(i))\n",
    "\n",
    "feat_max_mean_sort = np.argsort(-feat_max_sim_mean, axis=1)\n",
    "feat_avg_mean_sort = np.argsort(-feat_avg_sim_mean, axis=1)\n",
    "feat_max_avg_mean_sort = np.argsort(-feat_max_avg_sim_mean, axis=1)\n",
    "\n",
    "pd.DataFrame(feat_max_mean_sort).to_csv(\"./track3-feats/24689/sub2/feat_max_pred.csv\")\n",
    "pd.DataFrame(feat_avg_mean_sort).to_csv(\"./track3-feats/24689/sub2/feat_avg_pred.csv\")\n",
    "pd.DataFrame(feat_max_avg_mean_sort).to_csv(\"./track3-feats/24689/sub2/feat_max_avg_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}